{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are package to be loaded\n",
    "# Do not alter\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "from scipy.stats import chi2_contingency\n",
    "from scipy.stats import f_oneway"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 : Load the dataset into your environment.\n",
    "Load the dataset and inspect its basic structure. Pay attention to the data types of each column and check for any obvious issues (e.g., columns that should be numeric but are categorical)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "dataset = pd.read_csv(\"audience_targeting_dataset.csv\")\n",
    "\n",
    "# Display the first few rows of the dataset to understand its structure\n",
    "print(dataset.head())\n",
    "\n",
    "# Check for data types of each column\n",
    "print(dataset.dtypes)\n",
    "\n",
    "# Check for any missing values\n",
    "print(dataset.isnull().sum())\n",
    "\n",
    "# Get a summary of the dataset (like count, mean, std for numeric columns)\n",
    "print(dataset.describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 -  Explore the distribution of each feature.\n",
    "\n",
    "For numerical features, check how they are distributed (e.g., normal, skewed). For categorical features, see how frequently each category occurs. Visualizations like histograms and bar plots should help you here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Define numerical and categorical features based on your dataset\n",
    "numeric_features = ['Ad Engagement (Clicks/Views)', 'Time Spent on Platform (mins)', \n",
    "                    'Page Likes', 'Ad Impressions', 'Lookalike Score']  # Example numerical columns\n",
    "\n",
    "categorical_features = ['Age Range', 'Gender', 'Location', 'Language', 'Interest', \n",
    "                        'Device Type', 'Online Purchase Behavior', 'Recent Ad Interaction', \n",
    "                        'Ad Placement', 'Ad Format']  # Example categorical columns\n",
    "\n",
    "# Visualize the distribution of numerical features\n",
    "for feature in numeric_features:\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.histplot(dataset[feature], kde=True, bins=30)\n",
    "    plt.title(f'Distribution of {feature}')\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()\n",
    "\n",
    "# Visualize the distribution of categorical features\n",
    "for feature in categorical_features:\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.countplot(x=dataset[feature])\n",
    "    plt.title(f'Distribution of {feature}')\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel('Count')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 -  Assess the data for missing values, duplicates, and outliers.\n",
    "\n",
    "Look for any missing values and handle them appropriately (removal or imputation). Identify and remove any duplicates. For numerical features, check for outliers that could affect the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "missing_values = dataset.isnull().sum()\n",
    "print(f\"Missing Values per Feature:\\n{missing_values}\\n\")\n",
    "\n",
    "# Handle missing values - You can choose to drop or fill them\n",
    "# Example: Dropping rows with missing values\n",
    "dataset_cleaned = dataset.dropna()\n",
    "\n",
    "# Alternatively, you can fill missing values (e.g., with mean or mode)\n",
    "# dataset['Ad Impressions'] = dataset['Ad Impressions'].fillna(dataset['Ad Impressions'].mean())\n",
    "\n",
    "# Check for duplicates\n",
    "duplicates = dataset.duplicated().sum()\n",
    "print(f\"Number of duplicate rows: {duplicates}\")\n",
    "\n",
    "# Remove duplicates if found\n",
    "dataset_cleaned = dataset_cleaned.drop_duplicates()\n",
    "\n",
    "# Checking for outliers (for numerical features)\n",
    "# Using IQR (Interquartile Range) method to detect outliers\n",
    "Q1 = dataset_cleaned[numeric_features].quantile(0.25)\n",
    "Q3 = dataset_cleaned[numeric_features].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Define outliers as data points outside of 1.5*IQR range\n",
    "outliers = ((dataset_cleaned[numeric_features] < (Q1 - 1.5 * IQR)) | \n",
    "            (dataset_cleaned[numeric_features] > (Q3 + 1.5 * IQR)))\n",
    "\n",
    "# Count outliers for each feature\n",
    "outliers_count = outliers.sum()\n",
    "print(f\"Outliers detected in each numerical feature:\\n{outliers_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4 -  Select an appropriate metric for clustering performance.\n",
    "\n",
    "Think about which metric best reflects the quality of your segmentation. Metrics like Silhouette Score or Davies-Bouldin Index are common for clustering. Choose one that suits your task of identifying distinct customer segments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# First, let's assume we have applied a clustering algorithm (e.g., KMeans) on the data\n",
    "# Assuming 'X' is the data used for clustering (after preprocessing)\n",
    "X = dataset_cleaned[numeric_features]  # Example: using only numerical features for clustering\n",
    "\n",
    "# Apply KMeans clustering with a chosen number of clusters (e.g., 5)\n",
    "kmeans = KMeans(n_clusters=5, random_state=42)\n",
    "dataset_cleaned['Cluster'] = kmeans.fit_predict(X)\n",
    "\n",
    "# Calculate the Silhouette Score\n",
    "sil_score = silhouette_score(X, dataset_cleaned['Cluster'])\n",
    "print(f\"Silhouette Score for KMeans clustering: {sil_score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5 -  Create a baseline segmentation without machine learning.\n",
    "\n",
    "You can create a baseline by manually grouping customers based on simple rules such as their age range or location. For example, you could divide customers into segments like \"Young Adults,\" \"Middle-Aged,\" or \"Senior\" based on their age. Evaluate the baseline by checking the effectiveness of this approach in segmenting high-value customers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create baseline segmentation based on 'Age Range' and 'Location' (example)\n",
    "# Grouping based on simple rules (e.g., by 'Age Range' and 'Location')\n",
    "dataset_cleaned['Age Range Segment'] = dataset_cleaned['Age Range'].apply(lambda x: 'Young' if '18-25' in x else ('Middle-Aged' if '26-40' in x else 'Senior'))\n",
    "dataset_cleaned['Location Segment'] = dataset_cleaned['Location'].apply(lambda x: 'Urban' if 'City' in x else 'Rural')\n",
    "\n",
    "# Evaluate the baseline by examining the segments created\n",
    "# Example: Checking the distribution of these new segments\n",
    "print(\"Age Range Segmentation:\\n\", dataset_cleaned['Age Range Segment'].value_counts())\n",
    "print(\"Location Segmentation:\\n\", dataset_cleaned['Location Segment'].value_counts())\n",
    "\n",
    "# Optionally: Assess the average 'Lookalike Score' in each segment as a proxy for segment quality\n",
    "print(\"Average Lookalike Score by Age Range Segment:\\n\", dataset_cleaned.groupby('Age Range Segment')['Lookalike Score'].mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6 - Preprocess and encode the features to prepare for modeling.\n",
    "\n",
    "Before applying clustering or any machine learning algorithm, it's important to preprocess and encode the features. This involves handling numerical and categorical features differently: scaling numerical features and encoding categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Separate features into numerical and categorical\n",
    "numerical_features = ['Ad Engagement (Clicks/Views)', 'Time Spent on Platform (mins)', \n",
    "                      'Page Likes', 'Ad Impressions', 'Lookalike Score']  # Example numerical columns\n",
    "\n",
    "categorical_features = ['Age Range', 'Gender', 'Location', 'Language', 'Interest', \n",
    "                        'Device Type', 'Online Purchase Behavior', 'Recent Ad Interaction', \n",
    "                        'Ad Placement', 'Ad Format']  # Example categorical columns\n",
    "\n",
    "# Preprocessing for numerical data (scaling)\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),  # Handle missing values\n",
    "    ('scaler', StandardScaler())  # Standardize features\n",
    "])\n",
    "\n",
    "# Preprocessing for categorical data (one-hot encoding)\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),  # Handle missing values\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))  # One-hot encode categorical variables\n",
    "])\n",
    "\n",
    "# Combine preprocessing for both types of features\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Apply the preprocessing to the dataset\n",
    "X_preprocessed = preprocessor.fit_transform(dataset_cleaned)\n",
    "\n",
    "# Check the shape of the transformed data\n",
    "print(f\"Shape of preprocessed data: {X_preprocessed.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 7 - Fit a K-Means clustering model to your preprocessed data.\n",
    "\n",
    "Apply the K-Means algorithm to the data after preprocessing, specifying the number of clusters, and then assign cluster labels to each data point. Afterward, analyze the clustering results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Fit the KMeans model\n",
    "kmeans = KMeans(n_clusters=5, random_state=42)  # You can adjust n_clusters based on your dataset\n",
    "kmeans_labels = kmeans.fit_predict(X_preprocessed)  # X_preprocessed is the preprocessed data\n",
    "\n",
    "# Assign the cluster labels to the dataset\n",
    "dataset_cleaned['Cluster'] = kmeans_labels\n",
    "\n",
    "# Check the centroids of the clusters (optional)\n",
    "cluster_centroids = kmeans.cluster_centers_\n",
    "print(f\"Cluster Centroids:\\n{cluster_centroids}\")\n",
    "\n",
    "# Check the distribution of samples across clusters\n",
    "cluster_distribution = dataset_cleaned['Cluster'].value_counts()\n",
    "print(f\"Cluster Distribution:\\n{cluster_distribution}\")\n",
    "\n",
    "# Optionally, visualize the clusters using a 2D plot (e.g., first two principal components)\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Reduce the data to 2D using PCA for visualization\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_preprocessed)\n",
    "\n",
    "# Plotting the clusters\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=kmeans_labels, palette='viridis')\n",
    "plt.title(\"K-Means Clusters Visualized in 2D Space\")\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 8 - Tune the hyperparameters of the K-Means clustering model to improve the fit.\n",
    "\n",
    "The goal of this task is to optimize the number of clusters (n_clusters) for the K-Means algorithm. Use hyperparameter tuning techniques like GridSearchCV or RandomizedSearchCV to find the optimal number of clusters and any other important hyperparameters, such as initialization methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the range of hyperparameters to tune (e.g., n_clusters, init method)\n",
    "param_grid = {\n",
    "    'n_clusters': [3, 4, 5, 6, 7],  # Number of clusters to test\n",
    "    'init': ['k-means++', 'random'],  # Initialization methods for centroids\n",
    "    'max_iter': [300, 500],  # Maximum number of iterations\n",
    "    'n_init': [10, 20],  # Number of initializations\n",
    "}\n",
    "\n",
    "# Create a KMeans model\n",
    "kmeans = KMeans(random_state=42)\n",
    "\n",
    "# Set up GridSearchCV for hyperparameter tuning\n",
    "grid_search = GridSearchCV(estimator=kmeans, param_grid=param_grid, cv=3, n_jobs=-1, verbose=1)\n",
    "\n",
    "# Fit the model with the dataset\n",
    "grid_search.fit(X_preprocessed)\n",
    "\n",
    "# Print the best parameters found\n",
    "print(f\"Best Hyperparameters: {grid_search.best_params_}\")\n",
    "\n",
    "# Get the best estimator (model) based on the best parameters\n",
    "best_kmeans = grid_search.best_estimator_\n",
    "\n",
    "# Predict clusters using the best model\n",
    "best_labels = best_kmeans.predict(X_preprocessed)\n",
    "\n",
    "# Add the best cluster labels to the dataset\n",
    "dataset_cleaned['Optimized Cluster'] = best_labels\n",
    "\n",
    "# Check the distribution of samples across optimized clusters\n",
    "optimized_cluster_distribution = dataset_cleaned['Optimized Cluster'].value_counts()\n",
    "print(f\"Optimized Cluster Distribution:\\n{optimized_cluster_distribution}\")\n",
    "\n",
    "# Optional: Visualize the optimized clusters\n",
    "X_pca_optimized = PCA(n_components=2).fit_transform(X_preprocessed)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=X_pca_optimized[:, 0], y=X_pca_optimized[:, 1], hue=best_labels, palette='viridis')\n",
    "plt.title(\"Optimized K-Means Clusters Visualized in 2D Space\")\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 9 - Fit a DBSCAN\n",
    "\n",
    "Unlike K-Means, DBSCAN does not require the number of clusters to be specified. Instead, it groups points that are closely packed together, marking points in low-density regions as outliers. You need to choose appropriate hyperparameters like eps (maximum distance between two samples for one to be considered as in the neighborhood of the other) and min_samples (the number of samples in a neighborhood for a point to be considered a core point)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Fit DBSCAN model\n",
    "dbscan = DBSCAN(eps=0.5, min_samples=5)  # You can experiment with eps and min_samples\n",
    "dbscan_labels = dbscan.fit_predict(X_preprocessed)\n",
    "\n",
    "# Assign the DBSCAN cluster labels to the dataset\n",
    "dataset_cleaned['DBSCAN Cluster'] = dbscan_labels\n",
    "\n",
    "# Check the distribution of samples across DBSCAN clusters\n",
    "dbscan_cluster_distribution = dataset_cleaned['DBSCAN Cluster'].value_counts()\n",
    "print(f\"DBSCAN Cluster Distribution:\\n{dbscan_cluster_distribution}\")\n",
    "\n",
    "# Visualize the DBSCAN clusters\n",
    "X_pca_dbscan = PCA(n_components=2).fit_transform(X_preprocessed)\n",
    "\n",
    "# Plotting the DBSCAN clusters\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=X_pca_dbscan[:, 0], y=X_pca_dbscan[:, 1], hue=dbscan_labels, palette='viridis', marker='o')\n",
    "plt.title(\"DBSCAN Clusters Visualized in 2D Space\")\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 10 - Tune the hyperparameters of the DBSCAN model to improve its clustering performance.\n",
    "\n",
    "DBSCAN has two primary hyperparameters: eps (the maximum distance between two samples to be considered as neighbors) and min_samples (the number of samples required to form a dense region). You need to tune these hyperparameters using a search method like GridSearchCV or RandomizedSearchCV to find the optimal combination that improves the clustering results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "\n",
    "# Define the range of hyperparameters to tune for DBSCAN\n",
    "param_grid = {\n",
    "    'eps': np.linspace(0.1, 1.5, 15),  # Range of eps values to explore\n",
    "    'min_samples': [3, 5, 10, 15]  # Range of min_samples values to explore\n",
    "}\n",
    "\n",
    "# Create a DBSCAN model (GridSearchCV needs an estimator, so wrap DBSCAN in an estimator)\n",
    "class DBSCANEstimator:\n",
    "    def __init__(self, eps=0.5, min_samples=5):\n",
    "        self.eps = eps\n",
    "        self.min_samples = min_samples\n",
    "\n",
    "    def fit(self, X):\n",
    "        self.model = DBSCAN(eps=self.eps, min_samples=self.min_samples)\n",
    "        self.model.fit(X)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.model.labels_\n",
    "\n",
    "# Wrap DBSCAN in a GridSearch-compatible estimator\n",
    "dbscan_estimator = DBSCANEstimator()\n",
    "\n",
    "# Set up GridSearchCV for hyperparameter tuning (using the DBSCANEstimator)\n",
    "grid_search = GridSearchCV(estimator=dbscan_estimator, param_grid=param_grid, cv=3, n_jobs=-1, verbose=1)\n",
    "\n",
    "# Fit the GridSearchCV to the preprocessed data\n",
    "grid_search.fit(X_preprocessed)\n",
    "\n",
    "# Print the best parameters found\n",
    "print(f\"Best Hyperparameters: {grid_search.best_params_}\")\n",
    "\n",
    "# Get the best model (DBSCAN with optimal hyperparameters)\n",
    "best_dbscan = grid_search.best_estimator_\n",
    "\n",
    "# Predict clusters using the best model\n",
    "best_dbscan_labels = best_dbscan.predict(X_preprocessed)\n",
    "\n",
    "# Assign the best DBSCAN cluster labels to the dataset\n",
    "dataset_cleaned['Optimized DBSCAN Cluster'] = best_dbscan_labels\n",
    "\n",
    "# Check the distribution of samples across optimized DBSCAN clusters\n",
    "optimized_dbscan_cluster_distribution = dataset_cleaned['Optimized DBSCAN Cluster'].value_counts()\n",
    "print(f\"Optimized DBSCAN Cluster Distribution:\\n{optimized_dbscan_cluster_distribution}\")\n",
    "\n",
    "# Optional: Visualize the optimized DBSCAN clusters\n",
    "X_pca_optimized_dbscan = PCA(n_components=2).fit_transform(X_preprocessed)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=X_pca_optimized_dbscan[:, 0], y=X_pca_optimized_dbscan[:, 1], hue=best_dbscan_labels, palette='viridis')\n",
    "plt.title(\"Optimized DBSCAN Clusters Visualized in 2D Space\")\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 11 - Interpret the results from clustering (K-Means or DBSCAN) and identify which clusters correspond to high-value customers.\n",
    "\n",
    "The goal of this task is to understand the characteristics of each cluster and determine which clusters represent customers who are more likely to engage with ads and sign up for a credit card. You will analyze the features within each cluster and make decisions based on business criteria (e.g., high engagement, high purchase behavior).\n",
    "\n",
    "Analyze Cluster Characteristics: Examine the mean values of the features for each cluster to understand the differences. This will help identify clusters that have high ad engagement, time spent on the platform, and other features associated with high-value customers.\n",
    "\n",
    "Define High-Value Customer Criteria: Based on the cluster characteristics, define what constitutes a \"high-value\" customer. For example, you might decide that high engagement with ads (e.g., high Ad Engagement), high purchase behavior (e.g., Online Purchase Behavior), and higher Time Spent on Platform are indicators of a high-value customer.\n",
    "\n",
    "Segment High-Value Customers: Identify which clusters correspond to high-value customers by looking at the features that are high in certain clusters. Flag or label these customers as high-value.\n",
    "\n",
    "Visualize High-Value Customer Distribution: Optionally, visualize how these high-value customers are distributed across the clusters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Analyzing Cluster Characteristics\n",
    "# Calculate the mean values of each feature for each cluster\n",
    "cluster_means = dataset_cleaned.groupby('Optimized DBSCAN Cluster').mean()\n",
    "print(\"Cluster Characteristics (mean values for each feature):\")\n",
    "print(cluster_means)\n",
    "\n",
    "# Step 2: Define High-Value Customer Criteria\n",
    "# High-value customers are those with high Ad Engagement, high Online Purchase Behavior, and high Time Spent on Platform\n",
    "high_value_criteria = (dataset_cleaned['Ad Engagement (Clicks/Views)'] > dataset_cleaned['Ad Engagement (Clicks/Views)'].quantile(0.75)) & \\\n",
    "                       (dataset_cleaned['Online Purchase Behavior'] > dataset_cleaned['Online Purchase Behavior'].quantile(0.75)) & \\\n",
    "                       (dataset_cleaned['Time Spent on Platform (mins)'] > dataset_cleaned['Time Spent on Platform (mins)'].quantile(0.75))\n",
    "\n",
    "# Step 3: Segment High-Value Customers\n",
    "# Mark high-value customers\n",
    "dataset_cleaned['High Value Customer'] = np.where(high_value_criteria, 1, 0)\n",
    "\n",
    "# Identify which clusters contain the most high-value customers\n",
    "high_value_clusters = dataset_cleaned.groupby('Optimized DBSCAN Cluster')['High Value Customer'].sum()\n",
    "print(\"High-Value Customers by Cluster:\")\n",
    "print(high_value_clusters)\n",
    "\n",
    "# Step 4: Optional Visualization of High-Value Customers\n",
    "# Visualizing the high-value customers in the clusters\n",
    "X_pca_high_value = PCA(n_components=2).fit_transform(X_preprocessed)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=X_pca_high_value[:, 0], y=X_pca_high_value[:, 1], hue=dataset_cleaned['High Value Customer'], palette='coolwarm', marker='o')\n",
    "plt.title(\"High-Value Customers in Optimized DBSCAN Clusters Visualized in 2D Space\")\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After completing this task, you should be able to identify which clusters correspond to high-value customers based on their behavior. You can then use this information to target high-value customers with relevant ads and offers, increasing the likelihood of engagement and conversion for credit card sign-ups.\n",
    "\n",
    "This process involves interpreting clustering results, defining business-specific high-value criteria, and segmenting the customers accordingly. The optional visualization helps to understand how high-value customers are distributed across clusters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ads",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
