{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blood Donor Identification\n",
    "\n",
    "This project will teach you\n",
    "\n",
    "* how to analyze and clean data\n",
    "* how to identify predictive features and test hypothesis\n",
    "* fundamental classification metrics\n",
    "* training cycle of an ML model\n",
    "* fine-tuning Logistic Regression model\n",
    "\n",
    "The task we are solving for is to predict if a blood donor will make a donation in the upcoming month (here this month is March 2007) based on history of blood donations made by that donor.\n",
    "\n",
    "The project is organized in several section. Each section has a set of tasks for you to complete. <br>\n",
    "Please make sure to complete one task before moving onto the next\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are package to be loaded\n",
    "# Do not alter\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Cleaning\n",
    "\n",
    "### Task 1-1 : Load the data\n",
    "\n",
    "- **Description**: Load the data from file `train.csv` and assign it to variable `train_df`\n",
    "- **Code Instruction**: \n",
    "    1. Import dataset using the path `train.csv` and assign to `train_df`\n",
    "    2. From the column anmes, get all columns but the last one and assign it to `feats` as a list\n",
    "    3. Get the last column names and assign it to `label`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(data_folder + \"training_data.csv\")\n",
    "feats = list(train_df.columns[:-1])\n",
    "label = train_df.columns[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1-2: Understand the data\n",
    "\n",
    "- **Description**: In this task, have a look at a sample of train data and understand what the feature values looks like\n",
    "- **Code Instruction**: \n",
    "    1. Take a look at the top n rows of data\n",
    "    2. Get the summary statistics of the numeric features in the data\n",
    "    3. Get the distribution of feature values for categorical features in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Solution below, please remove \n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Solution below, please remove\n",
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1-3: Remove any duplicate rows\n",
    "\n",
    "- **Description**: In this task,remove any duplicate rows in the data\n",
    "- **Code Instruction**: Complete the function to return a datafarme with de-duplicated dataframe\n",
    "\n",
    "`Ask ChatGPT! : How does duplicate data impact performance of a Logistic Regression model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(df: pd.DataFrame) -> pd.DataFrame :\n",
    "    '''\n",
    "    Complete this function to return a de-duplicated dataframe\n",
    "    '''\n",
    "    \n",
    "    # TODO: remove the rest of code in this function\n",
    "    df = df.drop_duplicates()\n",
    "    return df\n",
    "\n",
    "\n",
    "# Do not change this code\n",
    "row_count = remove_duplicates(train_df).shape[0]\n",
    "print(row_count)\n",
    "remove_duplicates(train_df.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1-4: Fill Missing Values\n",
    "\n",
    "- **Description** : Fill any missing values in the data with column means for numerical cols and column mode for categorical columsn\n",
    "- **Code Instruction**: \n",
    "    1. Use `train_col_miss` to store mean/mode values for each numerical/categorical cols\n",
    "    2. Fill missing values with mean/mode in `train_col_miss`\n",
    "    3. Return a dataframe with missing values filled in\n",
    "\n",
    "`Ask ChatGPT! : How does missing values impact performance of a Logistic Regression model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_col_means = {}\n",
    "def fill_missing_value(df: pd.DataFrame, train=False) -> pd.DataFrame:\n",
    "    '''\n",
    "    Complete this function to fill missing (if there are)\n",
    "    with the mean value of the column\n",
    "\n",
    "    `train_col_mean` is a dictionary where keys are features\n",
    "    and values are mean of field\n",
    "\n",
    "    Hint: Use feats to iterate through columns\n",
    "    '''\n",
    "\n",
    "\n",
    "    # TODO: Solution below, please remove\n",
    "    for col in feats:\n",
    "        if train:\n",
    "            train_col_means[col] = df[col].mean()\n",
    "        \n",
    "        df[col] = df[col].fillna(train_col_means[col])\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Do not change this line of code\n",
    "fill_missing_value(train_df.copy(), train=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1-5: Identify outliers\n",
    "\n",
    "- **Description**: Compelete the below function to clip outlier using `Tukey Outlier method`. Replace the outlier with mean values calculated before.\n",
    "- **Code Instruction**:\n",
    "    1. Identify the numerical columns\n",
    "    2. Identify upper and lower bound using `Tukey Outlier method`\n",
    "    3. Replace outlier values using mean/model from `train_col_miss` dict defined earlier\n",
    "\n",
    "`Ask ChatGPT! : How does outlier impact performance of a Logistic Regression Model`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_col_bounds = {}\n",
    "def clip_outliers(df: pd.DataFrame, train=False) -> pd.DataFrame:\n",
    "    '''\n",
    "    Complete this function to get lower, upper bounds of each col\n",
    "    Replace low and high with mean values\n",
    "\n",
    "    `train_col_bounds` is a dictionary where key are features\n",
    "    and values are tuple (x,y) x being lower bound and y being higher bound\n",
    "\n",
    "    Hint: Use feats to iterate through columns\n",
    "    '''\n",
    "\n",
    "    # TODO: solution below, please remove\n",
    "    for col in feats:\n",
    "        if train:\n",
    "            p25, p75 = df[col].quantile([.25,.75])\n",
    "            iqr = p75 - p25\n",
    "            train_col_bounds[col] = (p25 - 1.5 * iqr, p75 + 1.5 * iqr)\n",
    "\n",
    "        df[col] = df[col].apply(lambda x: train_col_means[col] if (x < train_col_bounds[col][0] or x >  train_col_bounds[col][0]) else x)\n",
    "    print(train_col_bounds)\n",
    "\n",
    "\n",
    "# Do not change this code\n",
    "clip_outliers(train_df.copy(), train=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1-6: Identify Class Imbalance\n",
    "\n",
    "- **Description**: Complete this function to return the percentage of 0 labels in the data\n",
    "- **Code Instruction**:\n",
    "    1. Copmlete this function to return the percentage of 0-valued labels in the data\n",
    "\n",
    "`Ask ChatGPT! : How does imbalance impact performance of a Logistic Regression Model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_imbalance(df: pd.DataFrame) -> float:\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "\n",
    "    # TODO: Solution below, please remove\n",
    "    return df[label].value_counts(normalize=True)[1] * 100\n",
    "\n",
    "test_imbalance(train_df.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering\n",
    "\n",
    "### Task 2-1: Understand feature distributions\n",
    "\n",
    " - **Description**: If values across classes for a feature overlaps it tends to reduce Logistic Regression models predictive power. As such its a good idea to look at the distribution of feature values<br>\n",
    " - **Code Instruction**: \n",
    "    1. Get the column names for numeric fields in the train_df\n",
    "    2. For each field, plot one histogram each for each label value - showing the spread of the values. \n",
    "    3. See if any distribution tend to overlap quite a bit or not.\n",
    "\n",
    "`Ask ChatGPT: How does feature value overlap influence Logigisic Regression model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Solution below, please remove\n",
    "fig, axes = plt.subplots(2, 4, figsize=(12, 6))\n",
    "\n",
    "for i, feat in enumerate(feats):\n",
    "    row = i // 4\n",
    "    col = i % 4\n",
    "    train_df[train_df[label]==0][feat].hist(ax=axes[row, col])\n",
    "    train_df[train_df[label]==1][feat].hist(ax=axes[row+1, col])\n",
    "    axes[row, col].set_title(feat)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2-2: Feature Correlations for Numerical Features\n",
    "\n",
    "- **Description**: In this task, calculate pairwise correlation between features\n",
    "- **Code Instruction**:\n",
    "    1. Complete the function to calculate all pairwise correlation \n",
    "    (Hint: There is in inubilt-function in `pandas`for this)\n",
    "\n",
    "`Ask ChatGPT! : How do correlated feature impact performance of a Logistic Regression Model.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calc_corr(df: pd.DataFrame) -> None:\n",
    "    '''\n",
    "    Complete the function to calculate all pairwise correlation\n",
    "    From the output Identify the pair of features that are highly correlated.\n",
    "    '''\n",
    "    \n",
    "    # TODO: Solution below, please remove\n",
    "    return df.corr()\n",
    "\n",
    "\n",
    "# Do not change this code\n",
    "calc_corr(train_df[feats])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2-3: Identify Predictive Features\n",
    "\n",
    "- **Description**: Check which of the feature are predictive (i.e. will a donor donate blood). For this you can used a specific statistical test called 'Welch's t-test' and assuming significance alpha = 0.01\n",
    "- **Code Instruction**: Complete the following function\n",
    "    1. Return the feature name and p-value from t-test\n",
    "\n",
    "\n",
    "`Ask ChatGPT:  What is welch's t-test, and t-test - how does it help determing important features`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_student_ttest(df, col) -> tuple[str, float]:\n",
    "    '''\n",
    "    Write a function to return p-values from the Welch's t-test\n",
    "    for feature passed into the function\n",
    "    '''\n",
    "\n",
    "    # TODO: Solution below, please remove\n",
    "    t_stat, p_val = stats.ttest_ind(df[df[label]==0][col], df[df[label]==1][col], \n",
    "                                        equal_var=False)  # Welch's t-test\n",
    "    \n",
    "    return (col, p_val)\n",
    "\n",
    "for feat in feats:\n",
    "    run_student_ttest(train_df, feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2-4: Feature Engineering\n",
    "\n",
    "- **Description**: Lets see if there are any new featues we can engineering. Think about the feature 'Months since First Donation', and 'Number of Donations'.\n",
    "Can you figure out a sort of 'rate' feature from these two? Would it helpful in predicting who donate next month?\n",
    "- **Code Instruction**: Complete the following function to\n",
    "    1. Create a rate based feature and call it `Avg.Dontation Per Month`\n",
    "    2. Return dataframe with rate-based feature added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_new_feature(df) -> pd.DataFrame:\n",
    "    '''\n",
    "    Complete this function to create the 'rate' feature\n",
    "    '''\n",
    "\n",
    "    # TODO: Solution below, remove code \n",
    "    df['Avg.Dontation Per Month'] = df['Number of Donations']/(df['Months since First Donation'] - df['Months since Last Donation'] + 1)\n",
    "    return df\n",
    "\n",
    "\n",
    "# Do not change this code\n",
    "proc_df = create_new_feature(train_df.copy())\n",
    "feats_new = list(proc_df.columns)\n",
    "feats_new.remove(label)\n",
    "\n",
    "# lets have a look at the new feature set, is the new feature relevant?\n",
    "for feat in feats_new:\n",
    "    run_student_ttest(proc_df, feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2-5:  Removing irrelevant features\n",
    "\n",
    "- **Description**: Lets try removing irrelevant features and one of correlated feature\n",
    "- **Code Instruction**: Complete the following function to\n",
    "    1. drop the feature you deem irrelevant based on above code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_feature(df: pd.DataFrame, cols: list) -> pd.DataFrame:\n",
    "    '''\n",
    "    Create a function to remove the columns passed to this function\n",
    "    '''\n",
    "\n",
    "    # TODO: Solution below, please remove\n",
    "    df = df.drop(columns=cols)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Do not change this code\n",
    "proc_df = drop_feature(train_df.copy(), cols = ['Total Volume Donated (c.c.)', 'Months since First Donation'])\n",
    "proc_df.columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Understanding Classification Metrics\n",
    "\n",
    "The most common classification metrics are - \n",
    "* Accuraccy\n",
    "* Precision\n",
    "* Recall\n",
    "* F1-Score\n",
    "Let's Ask ChatGPT what there are - <br>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3-1: Choosing the right metric\n",
    "\n",
    "- **Description**: Now having analyzed the data (from Task 1-6), choose the best metrics for your task. \n",
    "Assume you got the following information from business - \n",
    "* If you prediction someone is going to donate, but they dont - this is huge concern. You want to reduce such `false positives` as much as possible.\n",
    "* If you predict someone is not going to donate, and they do come - it is ok. The blood donation camp can manage.\n",
    "Knowing the above - decide which metric to use. <br>\n",
    "Irrespective of what you use evaluate performance using F1 as well.\n",
    "\n",
    "- **Code Instruction**: \n",
    "    1. Compelete this function to calculate the metric you have chosen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_perf(y_act: list, y_pred: list) -> float:\n",
    "    '''\n",
    "    Compelete this function to calculate the metric\n",
    "    you have chosen\n",
    "    '''\n",
    "\n",
    "    # TODO: Solution below, pleas remove\n",
    "    val = precision_score(y_act, y_pred)\n",
    "    return val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build basline model\n",
    "\n",
    "Now, finally we can start training the model. When training an ML model its important to have three datasets\n",
    "* Train dataset - which you use to train the model and learn parameter\n",
    "* Validation dataset - the dataset to use to figure out which parameter are the best\n",
    "* Test dataset - the hidden dataset, that you DO NOT look at. Its only use to estimate the performance in future unseen datasets.\n",
    "\n",
    "### Task 4-1: Preproces the train data to create traininig and validation data\n",
    "\n",
    "- **Description** : In this task, we will create the datasets required for training the model\n",
    "- **Code Instruction**: \n",
    "    1. Load the train dataset \n",
    "    2. ONLY run the de-duplication function on train set\n",
    "    3. Encode categorical values\n",
    "    4. `Stratify` split the train dataset 80:20 to creatin a new train dataset and validation set\n",
    "\n",
    "`Ask ChatGPT: Why is it important to stratify when creating training and validation sets for imbalanced dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(df: pd.DataFrame) -> tuple[np.array, np.array, np. array, np.array]:\n",
    "    '''\n",
    "    Remove duplicate data from train file alone (using `remove_duplicates` used earlier)\n",
    "    Split train file data into train and valid set (keep in mind what we about imbalance learned in Task 5)\n",
    "        Hint use: train_test_split (set seed to 100), and use the `stratify` field\n",
    "        Ask ChatGPT: Why is it important to stratify when creating training and validation sets for imbalanced datasets\n",
    "    \n",
    "    Return np. arrays for train features, train labels, valid features, valid labels\n",
    "\n",
    "    '''\n",
    "\n",
    "    # TODO: Solution below, pleas remove\n",
    "    df = remove_duplicates(df)\n",
    "\n",
    "    label = 'Made Donation in March 2007'\n",
    "    feats = list(df.columns)\n",
    "    feats.remove(label)\n",
    "\n",
    "    X = df[feats].values\n",
    "    y = df[label].values\n",
    "\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, \n",
    "                                                          stratify=y, random_state=100)\n",
    "\n",
    "    return X_train, y_train, X_valid, y_valid\n",
    "\n",
    "\n",
    "# Do not change this\n",
    "X_train, y_train, X_valid, y_valid = create_dataset(pd.read_csv(data_folder + 'training_data.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4-2: Build Model\n",
    "\n",
    "- **Description**: Train a basline Logisitc Regression model with default parameters\n",
    "- **Code Instruction**: \n",
    "    1.  Complete this function to train a baseline LogisticRegression Model with default parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_base(X_train: np.array, y_train: np.array) -> LogisticRegression:\n",
    "    '''\n",
    "    Complete this function to\n",
    "    Train a baseline LogisticRegression Model with default parameter\n",
    "    Use random_state = 100 to keep results consistent\n",
    "    '''\n",
    "\n",
    "    # TODO: Solution below, please remove\n",
    "    model = LogisticRegression(random_state=100)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# Do not change this\n",
    "model = train_base(X_train, y_train)\n",
    "\n",
    "pred = model.predict(X_train)\n",
    "print(\"Train Performance\")\n",
    "print(\"Selected Metric: \", calc_perf(y_train, pred), \"F1-Score: \", f1_score(y_train, pred))\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "pred = model.predict(X_valid)\n",
    "print(\"Validation Performance\")\n",
    "print(\"Selected Metric: \", calc_perf(y_valid, pred),\"F1-Score: \", f1_score(y_valid, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.Improving baseline model\n",
    "\n",
    "### Task 5-1: Fix imbalance to improve performance\n",
    "\n",
    "- **Description**: Use sampling to balance the number positive and negative samples in the data\n",
    "- **Code Instruction**: Complete the following function to\n",
    "    1. Remove duplicate rows in data\n",
    "    2. Balance the number of positive and negative samples in train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rebalance_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    '''\n",
    "    Write a function to \n",
    "    (a) remove duplicate rows in data\n",
    "    (b) balance the number of positive and negative samples in train_data\n",
    "    Hint: Use downsampling, use random_state = 100\n",
    "\n",
    "    Return balance dataframe\n",
    "    '''\n",
    "\n",
    "\n",
    "    # TODO: Solution below, please remove\n",
    "    df = remove_duplicates(train_df)\n",
    "\n",
    "    pos_df = df[df[label] == 1]\n",
    "    neg_df = df[df[label] == 0]\n",
    "\n",
    "    df = pd.concat([neg_df.sample(frac=0.4, random_state=100), pos_df])\n",
    "    return df\n",
    "\n",
    "\n",
    "# Do not change the following code\n",
    "# load teh data\n",
    "train_df = pd.read_csv(data_folder + 'training_data.csv')\n",
    "print(train_df.shape, train_df[label].value_counts(normalize=True))\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "# balance the data\n",
    "train_df = rebalance_df(train_df)\n",
    "print(train_df.shape, train_df[label].value_counts(normalize=True))\n",
    "\n",
    "# check the performance with rebalance dataset\n",
    "print(\"\\n\\n\")\n",
    "X_train, y_train, X_valid, y_valid = create_dataset(train_df)\n",
    "model = train_base(X_train, y_train)\n",
    "print(X_train.shape)\n",
    "\n",
    "pred = model.predict(X_train)\n",
    "print(\"Train Performance\")\n",
    "print(\"Selected Metric: \", calc_perf(y_train, pred), \"F1-Score: \", f1_score(y_train, pred))\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "pred = model.predict(X_valid)\n",
    "print(\"Validation Performance\")\n",
    "print(\"Selected Metric: \", calc_perf(y_valid, pred),\"F1-Score: \", f1_score(y_valid, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5-2: Improve model using Class Weights\n",
    "\n",
    "- **Description**: Another way to combat data imbalance is configuring class weights in the Logistic Regression model <br>\n",
    "- **Code Instruction**: Write the function to train a Logistic Regression model\n",
    "    1. Use in-built parameters of Logistic Regression to weight minority mistake more\n",
    "\n",
    "`Ask ChatGPT: Why does one of the methods perform better than the other.`\n",
    "It could be related to the how the feature distribution for both class overlap as seen in Task 2-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_tune_model(X_train: np.array, y_train: np.array):\n",
    "    '''\n",
    "    Write the function to train a Logistic Regression model\n",
    "    and use `class_weight` parameter\n",
    "    Use random_state = 100 to keep results consistent\n",
    "    '''\n",
    "\n",
    "    # TODO: Solution below, please remove code\n",
    "    model = LogisticRegression(class_weight={0:0.25, 1:0.75 }, random_state=100)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# do no change the following code\n",
    "train_df = pd.read_csv(data_folder + 'training_data.csv')\n",
    "X_train, y_train, X_valid, y_valid = create_dataset(train_df)\n",
    "\n",
    "model = train_tune_model(X_train, y_train)\n",
    "\n",
    "pred = model.predict(X_train)\n",
    "print(\"Train Performance\")\n",
    "print(\"Selected Metric: \", calc_perf(y_train, pred), \"F1-Score: \", f1_score(y_train, pred))\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "pred = model.predict(X_valid)\n",
    "print(\"Validation Performance\")\n",
    "print(\"Selected Metric: \", calc_perf(y_valid, pred),\"F1-Score: \", f1_score(y_valid, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5-3:  Focus on important features\n",
    "\n",
    "- **Description**: Play around with the feature set passed to the model to ifnd the optimal one.\n",
    "--**Code Instruction**: \n",
    "    1. Use `create_new_feature` to create rate feature.\n",
    "    2. Use `drop_feature` to remove featuer you deem unneccessary\n",
    "\n",
    "How do all this influence performance - which feature set do you finally keep?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not chage this code\n",
    "train_df = pd.read_csv(data_folder + 'training_data.csv')\n",
    "train_df = rebalance_df(train_df)\n",
    "\n",
    "# ADD YOUR CODE HERE\n",
    "# Hint: use `create_new_feature` and `drop_feature` functions mentioned before\n",
    "# TODO: Solution below, remove it\n",
    "train_df = create_new_feature(train_df)\n",
    "train_df = drop_feature(train_df, cols = ['Total Volume Donated (c.c.)'])\n",
    "\n",
    "\n",
    "# Do not change this code\n",
    "X_train, y_train, X_valid, y_valid = create_dataset(train_df)\n",
    "model = train_base(X_train, y_train)\n",
    "\n",
    "pred = model.predict(X_train)\n",
    "print(\"Train Performance\")\n",
    "print(\"Selected Metric: \", calc_perf(y_train, pred), \"F1-Score: \", f1_score(y_train, pred))\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "pred = model.predict(X_valid)\n",
    "print(\"Validation Performance\")\n",
    "print(\"Selected Metric: \", calc_perf(y_valid, pred),\"F1-Score: \", f1_score(y_valid, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Did the performance drop? If so, why? <br>\n",
    "When you add which feature back in is the performance coming back up? <br>\n",
    "\n",
    "Does it make sense that adding a feature that failed student t-test helped improve performance of Logistic Regression? <br>\n",
    "\n",
    "`Ask ChatGPT why this could happen (Hint it could be related to how features work together)`\n",
    "\n",
    "### Task 5-4: Normalizing Features\n",
    "\n",
    "- **Describe**: Normalize features to see how it impact perforamance for the Log.Reg model from the task\n",
    "- **Code Instruction** Complete the function to\n",
    "    1. Use standard scaling to normalize features \n",
    "\n",
    "`Ask ChatGPT what type of feature scaling is best for Logistic Regression and why`\n",
    "\n",
    "### Bonus [Optional] Task\n",
    "Fine-tune the Logistic Regression Model.\n",
    "Some of the parameters you may want to experiment with are - solver, penatly and C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train_tune_model(X_train: np.array, y_train: np.array, \n",
    "                     scaler):\n",
    "    '''\n",
    "    Complete this function to normalize features \n",
    "    X_train: is the train features values\n",
    "    y_train: is train labels\n",
    "    scaler: The scaler you have chosen\n",
    "\n",
    "    Bonus [Optional] Task : \n",
    "    Fine-tune the Logistic Regression Model.\n",
    "    Some of the parameters you may want to experiment with are - solver, penatly and C\n",
    "    '''\n",
    "\n",
    "    # Hint: Use StandardScaler to normalize features\n",
    "    # Ask ChatGPT what type of feature scaling is best for Logistic Regression and why\n",
    "\n",
    "    # TODO: Solution below, remove this code\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "\n",
    "    model = LogisticRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    return (model, scaler)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "\n",
    "# Do not change this code\n",
    "train_df = pd.read_csv(data_folder + \"training_data.csv\")\n",
    "train_df = rebalance_df(train_df)\n",
    "train_df = drop_feature(train_df, cols = ['Total Volume Donated (c.c.)'])\n",
    "\n",
    "X_train, y_train, X_valid, y_valid = create_dataset(train_df)\n",
    "model, scaler = train_tune_model(X_train, y_train, scaler)\n",
    "\n",
    "X_valid = scaler.transform(X_valid) # we use the same scaler you have used earlier\n",
    "pred = model.predict(X_valid)\n",
    "print(\"Validation Performance\")\n",
    "print(\"Selected Metric: \", calc_perf(y_valid, pred),\"F1-Score: \", f1_score(y_valid, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5-5: Model Interpretability\n",
    "\n",
    "- **Description**: Extract the coefficient from the model by completing this function\n",
    "- **Code Instruction**: Complete the following function to \n",
    "    1. Print thte coefficient for each feature\n",
    "    2. Print the intercept for the Log.Reg model as well\n",
    "\n",
    "`Ask ChatGPT: How do you interpret coeffcient of a Logistic Regression model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_coeff(model : LogisticRegression, feats: list):\n",
    "    '''\n",
    "    Complete this function to print pair of values\n",
    "    (feature name, coeff value)\n",
    "    '''\n",
    "\n",
    "    # TODO: Remove this code\n",
    "    coeffs = list(model.coef_)[0]\n",
    "    print(coeffs)\n",
    "\n",
    "    for i in range(len(feats)):\n",
    "        print(feats[i], coeffs[i])\n",
    "    \n",
    "    print(\"Intercept\", model.intercept_)\n",
    "\n",
    "\n",
    "get_model_coeff(model, list(train_df.columns)[:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5-6 - Final Prediction\n",
    "\n",
    "- **Description**:Finally apply all the transformation you deem best on the data and the best model you found to get prediction on test datasets\n",
    "- **Code Instruction**:\n",
    "    1. Load test data\n",
    "    2. Look at code written till now - missing value, outliers, encoding, scaling - apply all\n",
    "    3. Predict the class using the best model trained from earlier tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(data_folder + 'test_data.csv')\n",
    "test_data.head()\n",
    "print(test_data.columns)\n",
    "print(feats)\n",
    "\n",
    "def get_test_prediction(test_df: pd.DataFrame) -> list:\n",
    "    '''\n",
    "    Apply all the data transformation you deem necessary\n",
    "    Note: \n",
    "    Missing value must be replaced by mean value from train set NOT test set\n",
    "\n",
    "    Fit model on train data and get prediction on test set\n",
    "    '''\n",
    "\n",
    "    feats = ['Months since Last Donation', 'Number of Donations', 'Months since First Donation']\n",
    "    X_test = test_df[feats]\n",
    "    X_test = scaler.transform(X_test)\n",
    "    val = list(model.predict(X_test))\n",
    "\n",
    "    return val\n",
    "\n",
    "get_test_prediction(test_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ads",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
