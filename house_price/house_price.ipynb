{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Housing Price Prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Module 1\n",
    "\n",
    "### Task 1-1 : Load the data\n",
    "\n",
    "- **Description**: Load the data from file `train.csv` and assign it to variable `train_df`\n",
    "- **Code Instruction**: \n",
    "    1. Import dataset using the path `train.csv` and assign to `train_df`\n",
    "    2. From the column anmes, get all columns but the last one and assign it to `feats` as a list\n",
    "    3. Get the last column names and assign it to `label`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"train.csv\"  # Update with your dataset path\n",
    "data = pd.read_csv(file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1-2: Splitting the Data into Train and Test Sets\n",
    "\n",
    "- **Description**: Create a train and test dataset from the dataset\n",
    "- **Code Instruction**: \n",
    "    1. Load features in `X` and labels into `y`\n",
    "    2. Split dataset into train and test using `train_test_split`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Separate features and target variable\n",
    "X = data.drop(columns=['Price'])  # Features\n",
    "y = data['Price']  # Target variable\n",
    "\n",
    "# Split the data into training and testing sets (80/20 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Display the shapes of the training and testing sets\n",
    "print(\"Training set size (features):\", X_train.shape)\n",
    "print(\"Training set size (target):\", y_train.shape)\n",
    "print(\"Testing set size (features):\", X_test.shape)\n",
    "print(\"Testing set size (target):\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1-3: Viewing Summary Statistics\n",
    "\n",
    "- **Description**: The first step in checking the quality of your training data is to view its summary statistics\n",
    "- **Code Instruction**:\n",
    "    1. See top 5 rows of data\n",
    "    2. See summary statistics of numeric features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display summary statistics of numerical features in the training dataset\n",
    "print(\"Summary statistics of numerical features in the training data:\")\n",
    "numerical_features = X_train.select_dtypes(include=['float64', 'int64']).columns\n",
    "print(X_train[numerical_features].describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1-4: Checking for Missing Values\n",
    "- **Description**: In this task,remove any duplicate rows in the data\n",
    "- **Code Instruction**: Calculate number of values in each columns and store in `missing_values` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in the training dataset\n",
    "print(\"\\nMissing values in the training dataset:\")\n",
    "missing_values = X_train.isnull().sum()\n",
    "missing_columns = missing_values[missing_values > 0]\n",
    "print(missing_columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1-5: Checking for Duplicate Values\n",
    "\n",
    "- **Description**: Detect duplicate rows in the training dataset\n",
    "- **Code Instruction**: Store number of duplicate rows in `duplicates`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate rows in the training dataset\n",
    "duplicates = X_train.duplicated().sum()\n",
    "print(f\"\\nNumber of duplicate rows in the training dataset: {duplicates}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1-6: Checking for Outliers Using the Tukey Outlier Rule\n",
    "\n",
    "- **Description**: Identify outliers in the numerical features using Tukey's rule, which defines an outlier as a value beyond 1.5 times the interquartile range (IQR) above the 75th percentile or below the 25th percentile.\n",
    "- **Code Insturction**:\n",
    "    1. Complete function to return number of outliers in each field\n",
    "    2. Stor in `num_outliers` number of outlier for each field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify outliers using Tukey's rule\n",
    "def tukey_outliers(column):\n",
    "    q1 = column.quantile(0.25)  # 25th percentile\n",
    "    q3 = column.quantile(0.75)  # 75th percentile\n",
    "    iqr = q3 - q1  # Interquartile range\n",
    "    lower_bound = q1 - 1.5 * iqr\n",
    "    upper_bound = q3 + 1.5 * iqr\n",
    "    outliers = (column < lower_bound) | (column > upper_bound)\n",
    "    return outliers.sum()\n",
    "\n",
    "print(\"\\nOutliers in each numerical feature using Tukey's rule:\")\n",
    "for col in numerical_features:\n",
    "    num_outliers = tukey_outliers(X_train[col])\n",
    "    print(f\"{col}: {num_outliers} outliers\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Modelling\n",
    "\n",
    "### Task 2-1: Understanding the Data - Feature Value Distributions\n",
    "\n",
    "- **Description**: In this task, students will explore the distributions of features in the dataset. Understanding feature distributions helps identify patterns, skewness, and irregularities (e.g., heavy tails). Use visualization techniques to analyze both numerical and categorical features.\n",
    "- **Code Instruction**:\n",
    "    1. Visualize distributions of numerical features using historgrams\n",
    "    2. Visualize distributions of categorical features using bar plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Visualize distributions of numerical features\n",
    "print(\"Visualizing distributions of numerical features:\")\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i, col in enumerate(numerical_features, 1):\n",
    "    plt.subplot(3, 3, i)\n",
    "    sns.histplot(X_train[col], kde=True, bins=30)\n",
    "    plt.title(f\"Distribution of {col}\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Visualize distributions of categorical features\n",
    "categorical_features = X_train.select_dtypes(include=['object', 'category']).columns\n",
    "print(\"\\nVisualizing distributions of categorical features:\")\n",
    "plt.figure(figsize=(15, 8))\n",
    "for i, col in enumerate(categorical_features, 1):\n",
    "    plt.subplot(2, 2, i)\n",
    "    sns.countplot(x=X_train[col], order=X_train[col].value_counts().index)\n",
    "    plt.title(f\"Distribution of {col}\")\n",
    "    plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2-2: Building a Baseline No-Machine Learning Solution\n",
    "\n",
    "- **Description**: In this task, students will create a simple baseline solution to predict housing prices without using machine learning. The baseline will use a basic statistical approach, such as predicting the mean of the target variable (Price)\n",
    "- **Code Instruction**:\n",
    "    1. Predict using mean of train labels\n",
    "    2. Calculate performance using mse, mae, rmse, and r2\n",
    "\n",
    "`Ask ChatGPT: Please explain mse, mae, rmse, and r2 and their difference with examples`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Baseline solution: Predicting the mean price\n",
    "baseline_prediction = y_train.mean()\n",
    "\n",
    "# Generate baseline predictions (same value for all)\n",
    "y_pred_baseline = np.full_like(y_test, baseline_prediction)\n",
    "\n",
    "# Evaluate baseline performance\n",
    "mae_baseline = mean_absolute_error(y_test, y_pred_baseline)\n",
    "mse_baseline = mean_squared_error(y_test, y_pred_baseline)\n",
    "rmse_baseline = np.sqrt(mse_baseline)\n",
    "r2_baseline = r2_score(y_test, y_pred_baseline)\n",
    "\n",
    "# Display results\n",
    "print(\"Baseline Model Performance:\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae_baseline:.2f}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse_baseline:.2f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse_baseline:.2f}\")\n",
    "print(f\"R-squared (R²): {r2_baseline:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2-3: Building a Basic Linear Regression Model\n",
    "\n",
    "-**Description**: In this task, students will implement a basic linear regression model. They will preprocess the data (e.g., encode categorical features, normalize numerical features) to ensure compatibility with the linear regression algorithm. This task introduces foundational concepts in model building and data transformation.\n",
    "- **Code Instruction**:\n",
    "    1. Use `pipeline` from sklean to put together transformations\n",
    "    2. Use mean imputing to fill missing values and standard scaling for numerical values\n",
    "    3. Use mode imputring to fill missing value and One Hot Encoder for categorical values\n",
    "    4. Use Column Transformer to tranform subset of columns\n",
    "    5. Fit the default Linear Regression model\n",
    "    6. Evaluate performance on baslines LR model on mse, mae, rmse, and r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Separate categorical and numerical features\n",
    "categorical_features = X_train.select_dtypes(include=['object', 'category']).columns\n",
    "numerical_features = X_train.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\")),  # Fill missing values with the mean\n",
    "    (\"scaler\", StandardScaler())                 # Scale the features\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),  # Fill missing values with the most frequent value\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))     # One-hot encode the categorical features\n",
    "])\n",
    "\n",
    "\n",
    "# Preprocessing for numerical and categorical features\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Define the linear regression pipeline\n",
    "linear_regression_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', LinearRegression())\n",
    "])\n",
    "\n",
    "# Fit the model\n",
    "linear_regression_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = linear_regression_pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Display results\n",
    "print(\"Linear Regression Model Performance:\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")\n",
    "print(f\"R-squared (R²): {r2:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2-4: Comparing LR with No-Machine Learning Solution\n",
    "\n",
    "- **Description**: Compare performance of linear regression model and no-ML baseline model\n",
    "- **Code Instruction**: \n",
    "    1. Evaluate performance of baslines model on mse, mae, rmse, and r2\n",
    "    2. Evaluate performance on LR model on mse, mae, rmse, and r2\n",
    "    3. See which one is better!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate baseline performance (from Task 7)\n",
    "y_pred_baseline = np.full_like(y_test, y_train.mean())  # Baseline: Predict the mean price\n",
    "\n",
    "# Calculate metrics for the baseline\n",
    "mae_baseline = mean_absolute_error(y_test, y_pred_baseline)\n",
    "mse_baseline = mean_squared_error(y_test, y_pred_baseline)\n",
    "rmse_baseline = np.sqrt(mse_baseline)\n",
    "r2_baseline = r2_score(y_test, y_pred_baseline)\n",
    "\n",
    "# Evaluate linear regression model performance\n",
    "mae_lr = mean_absolute_error(y_test, y_pred)\n",
    "mse_lr = mean_squared_error(y_test, y_pred)\n",
    "rmse_lr = np.sqrt(mse_lr)\n",
    "r2_lr = r2_score(y_test, y_pred)\n",
    "\n",
    "# Display comparison between baseline and linear regression model\n",
    "print(\"Performance Comparison (Baseline vs Linear Regression):\")\n",
    "print(\"\\nBaseline Model:\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae_baseline:.2f}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse_baseline:.2f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse_baseline:.2f}\")\n",
    "print(f\"R-squared (R²): {r2_baseline:.2f}\")\n",
    "\n",
    "print(\"\\nLinear Regression Model:\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae_lr:.2f}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse_lr:.2f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse_lr:.2f}\")\n",
    "print(f\"R-squared (R²): {r2_lr:.2f}\")\n",
    "\n",
    "# Visual comparison (optional)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(['Baseline', 'Linear Regression'], [mae_baseline, mae_lr], color=['orange', 'blue'])\n",
    "plt.ylabel(\"Mean Absolute Error (MAE)\")\n",
    "plt.title(\"Comparison of MAE between Baseline and Linear Regression\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ads",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
