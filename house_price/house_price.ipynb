{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Housing Price Prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Loading the Data\n",
    "\n",
    "Your first task is to load the dataset into a Pandas DataFrame. The dataset is in CSV format and contains information about housing prices and their associated features. Use Python's data-handling libraries to load the data and inspect the first few rows to ensure the data was loaded correctly. This task introduces you to working with external data files and familiarizing yourself with their structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"housing_prices_dataset.csv\"  # Update with your dataset path\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(\"First 5 rows of the dataset:\")\n",
    "print(data.head())\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(\"\\nBasic Information about the dataset:\")\n",
    "print(data.info())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Splitting the Data into Train and Test Sets\n",
    "\n",
    "Your second task is to split the dataset into training and testing sets. The training set will be used to build your machine learning model, and the testing set will evaluate its performance on unseen data. Use an 80/20 split for this task. Ensure that the target variable (Price) is separated from the features, as we will be predicting this variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Separate features and target variable\n",
    "X = data.drop(columns=['Price'])  # Features\n",
    "y = data['Price']  # Target variable\n",
    "\n",
    "# Split the data into training and testing sets (80/20 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Display the shapes of the training and testing sets\n",
    "print(\"Training set size (features):\", X_train.shape)\n",
    "print(\"Training set size (target):\", y_train.shape)\n",
    "print(\"Testing set size (features):\", X_test.shape)\n",
    "print(\"Testing set size (target):\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3a: Viewing Summary Statistics\n",
    "The first step in checking the quality of your training data is to view its summary statistics. This helps you understand the range, central tendency (mean/median), and variability (standard deviation) of each numerical feature. It also gives clues about potential data issues, such as extreme values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display summary statistics of numerical features in the training dataset\n",
    "print(\"Summary statistics of numerical features in the training data:\")\n",
    "numerical_features = X_train.select_dtypes(include=['float64', 'int64']).columns\n",
    "print(X_train[numerical_features].describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3b: Checking for Missing Values\n",
    "Identify which columns in the training dataset contain missing values and how many. This step is essential because missing values can disrupt model training and need to be addressed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in the training dataset\n",
    "print(\"\\nMissing values in the training dataset:\")\n",
    "missing_values = X_train.isnull().sum()\n",
    "missing_columns = missing_values[missing_values > 0]\n",
    "print(missing_columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3c: Checking for Duplicate Values\n",
    "Task Description:\n",
    "Detect duplicate rows in the training dataset. Duplicates can distort the model by over-representing certain patterns, leading to bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate rows in the training dataset\n",
    "duplicates = X_train.duplicated().sum()\n",
    "print(f\"\\nNumber of duplicate rows in the training dataset: {duplicates}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3d: Checking for Outliers Using the Tukey Outlier Rule\n",
    "\n",
    "Identify outliers in the numerical features using Tukey's rule, which defines an outlier as a value beyond 1.5 times the interquartile range (IQR) above the 75th percentile or below the 25th percentile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify outliers using Tukey's rule\n",
    "def tukey_outliers(column):\n",
    "    q1 = column.quantile(0.25)  # 25th percentile\n",
    "    q3 = column.quantile(0.75)  # 75th percentile\n",
    "    iqr = q3 - q1  # Interquartile range\n",
    "    lower_bound = q1 - 1.5 * iqr\n",
    "    upper_bound = q3 + 1.5 * iqr\n",
    "    outliers = (column < lower_bound) | (column > upper_bound)\n",
    "    return outliers.sum()\n",
    "\n",
    "print(\"\\nOutliers in each numerical feature using Tukey's rule:\")\n",
    "for col in numerical_features:\n",
    "    num_outliers = tukey_outliers(X_train[col])\n",
    "    print(f\"{col}: {num_outliers} outliers\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Understanding the Data - Feature Value Distributions\n",
    "In this task, students will explore the distributions of features in the dataset. Understanding feature distributions helps identify patterns, skewness, and irregularities (e.g., heavy tails). Use visualization techniques to analyze both numerical and categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Visualize distributions of numerical features\n",
    "print(\"Visualizing distributions of numerical features:\")\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i, col in enumerate(numerical_features, 1):\n",
    "    plt.subplot(3, 3, i)\n",
    "    sns.histplot(X_train[col], kde=True, bins=30)\n",
    "    plt.title(f\"Distribution of {col}\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Visualize distributions of categorical features\n",
    "categorical_features = X_train.select_dtypes(include=['object', 'category']).columns\n",
    "print(\"\\nVisualizing distributions of categorical features:\")\n",
    "plt.figure(figsize=(15, 8))\n",
    "for i, col in enumerate(categorical_features, 1):\n",
    "    plt.subplot(2, 2, i)\n",
    "    sns.countplot(x=X_train[col], order=X_train[col].value_counts().index)\n",
    "    plt.title(f\"Distribution of {col}\")\n",
    "    plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: Understanding How to Convert the Problem of Predicting Price to a Regression Problem\n",
    "In this task, students will identify how the problem of predicting housing prices fits into the category of regression problems. They will verify that the target variable (Price) is continuous and understand why regression models are suitable. This task lays the theoretical foundation for model selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Check the data type and distribution of the target variable\n",
    "print(\"Target variable type:\", y_train.dtype)\n",
    "print(\"\\nSummary statistics of the target variable:\")\n",
    "print(y_train.describe())\n",
    "\n",
    "# Visualize the distribution of the target variable\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(y_train, kde=True, bins=30, color='blue')\n",
    "plt.title(\"Distribution of Target Variable: Price\")\n",
    "plt.xlabel(\"Price\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6: Figuring Out What Metric to Choose - Pros and Cons of Each\n",
    "Students will explore different metrics for evaluating regression models, including their strengths and weaknesses. The task involves understanding common metrics such as Mean Absolute Error (MAE), Mean Squared Error (MSE), and R-squared (R²), and deciding which metric(s) to use for this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# Dummy predictions to demonstrate metrics (replace with actual model predictions later)\n",
    "dummy_predictions = y_train.mean()  # Using mean of the target as a baseline\n",
    "y_pred_dummy = np.full_like(y_train, dummy_predictions)\n",
    "\n",
    "# Calculate metrics\n",
    "mae = mean_absolute_error(y_train, y_pred_dummy)\n",
    "mse = mean_squared_error(y_train, y_pred_dummy)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_train, y_pred_dummy)\n",
    "\n",
    "# Display results\n",
    "print(\"Evaluation Metrics for Dummy Predictions:\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")\n",
    "print(f\"R-squared (R²): {r2:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 7: Building a Baseline No-Machine Learning Solution\n",
    "In this task, students will create a simple baseline solution to predict housing prices without using machine learning. The baseline will use a basic statistical approach, such as predicting the mean or median of the target variable (Price). This provides a reference point for evaluating the performance of machine learning models.\n",
    "\n",
    "`Note: You can evaluate the model based on which metric you found the most effective.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Baseline solution: Predicting the mean price\n",
    "baseline_prediction = y_train.mean()\n",
    "\n",
    "# Generate baseline predictions (same value for all)\n",
    "y_pred_baseline = np.full_like(y_test, baseline_prediction)\n",
    "\n",
    "# Evaluate baseline performance\n",
    "mae_baseline = mean_absolute_error(y_test, y_pred_baseline)\n",
    "mse_baseline = mean_squared_error(y_test, y_pred_baseline)\n",
    "rmse_baseline = np.sqrt(mse_baseline)\n",
    "r2_baseline = r2_score(y_test, y_pred_baseline)\n",
    "\n",
    "# Display results\n",
    "print(\"Baseline Model Performance:\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae_baseline:.2f}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse_baseline:.2f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse_baseline:.2f}\")\n",
    "print(f\"R-squared (R²): {r2_baseline:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 8: Building a Basic Linear Regression Model\n",
    "In this task, students will implement a basic linear regression model. They will preprocess the data (e.g., encode categorical features, normalize numerical features) to ensure compatibility with the linear regression algorithm. This task introduces foundational concepts in model building and data transformation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Separate categorical and numerical features\n",
    "categorical_features = X_train.select_dtypes(include=['object', 'category']).columns\n",
    "numerical_features = X_train.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "# Preprocessing for numerical and categorical features\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "    ])\n",
    "\n",
    "# Define the linear regression pipeline\n",
    "linear_regression_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', LinearRegression())\n",
    "])\n",
    "\n",
    "# Fit the model\n",
    "linear_regression_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = linear_regression_pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Display results\n",
    "print(\"Linear Regression Model Performance:\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")\n",
    "print(f\"R-squared (R²): {r2:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 9: Evaluating Performance of Model Using Chosen Metric and Comparing with No-Machine Learning Solution\n",
    "\n",
    "In this task, students will evaluate the performance of the linear regression model they built in the previous task. They will compare the performance of this machine learning model against the baseline (no-machine learning solution) using the chosen evaluation metrics (MAE, MSE, RMSE, R²). This comparison will help them understand whether the model provides a meaningful improvement over a simple statistical approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate baseline performance (from Task 7)\n",
    "y_pred_baseline = np.full_like(y_test, y_train.mean())  # Baseline: Predict the mean price\n",
    "\n",
    "# Calculate metrics for the baseline\n",
    "mae_baseline = mean_absolute_error(y_test, y_pred_baseline)\n",
    "mse_baseline = mean_squared_error(y_test, y_pred_baseline)\n",
    "rmse_baseline = np.sqrt(mse_baseline)\n",
    "r2_baseline = r2_score(y_test, y_pred_baseline)\n",
    "\n",
    "# Evaluate linear regression model performance\n",
    "mae_lr = mean_absolute_error(y_test, y_pred)\n",
    "mse_lr = mean_squared_error(y_test, y_pred)\n",
    "rmse_lr = np.sqrt(mse_lr)\n",
    "r2_lr = r2_score(y_test, y_pred)\n",
    "\n",
    "# Display comparison between baseline and linear regression model\n",
    "print(\"Performance Comparison (Baseline vs Linear Regression):\")\n",
    "print(\"\\nBaseline Model:\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae_baseline:.2f}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse_baseline:.2f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse_baseline:.2f}\")\n",
    "print(f\"R-squared (R²): {r2_baseline:.2f}\")\n",
    "\n",
    "print(\"\\nLinear Regression Model:\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae_lr:.2f}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse_lr:.2f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse_lr:.2f}\")\n",
    "print(f\"R-squared (R²): {r2_lr:.2f}\")\n",
    "\n",
    "# Visual comparison (optional)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(['Baseline', 'Linear Regression'], [mae_baseline, mae_lr], color=['orange', 'blue'])\n",
    "plt.ylabel(\"Mean Absolute Error (MAE)\")\n",
    "plt.title(\"Comparison of MAE between Baseline and Linear Regression\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ads",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
