{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Telephone Subscription Prediction.\n",
    "\n",
    "This project requires you to\n",
    "\n",
    "* Analyze and clean data\n",
    "* Identify predictive features and test hypothesis\n",
    "* Choose between fundamental classification metrics\n",
    "* Fit and fine-tune a logreg and xgboost model for prediction\n",
    "\n",
    "The task we are solving for is to predict if customer will subscribe to telephone service or not.\n",
    "\n",
    "The project is organized in several Modules. Each Module has a set of tasks for you to complete. <br>\n",
    "Please make sure to complete one task before moving onto the next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are package to be loaded\n",
    "# Do not alter\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "from scipy.stats import chi2_contingency\n",
    "from scipy.stats import f_oneway"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module 1\n",
    "\n",
    "### Task 1-1 : Load the data\n",
    "\n",
    "- **Description**: Load the data from file `train.csv` and assign it to variable `train_df`\n",
    "- **Code Instruction**: \n",
    "    1. Import dataset using the path `train.csv` and assign to `train_df`\n",
    "    2. From the column anmes, get all columns but the last one and assign it to `feats` as a list\n",
    "    3. Get the last column names and assign it to `label`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"train.csv\")\n",
    "feats = list(train_df.columns[:-1])\n",
    "label = train_df.columns[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1-2: Understand the data\n",
    "\n",
    "- **Description**: In this task, have a look at a sample of train data and understand what the feature values looks like\n",
    "- **Code Instruction**: \n",
    "    1. Take a look at the top n rows of data\n",
    "    2. Get the summary statistics of the numeric features in the data\n",
    "    3. Get the distribution of feature values for categorical features in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Solution below, please remove \n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Solution below, please remove\n",
    "\n",
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = ['age','balance','duration']\n",
    "cat_cols = ['job','marital','education','default','housing','loan','contact','month','day','campaign','pdays','previous','poutcome']\n",
    "label = 'subscribed'\n",
    "\n",
    "for col in cat_cols:\n",
    "    print(train_df[col].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1-3: Remove any duplicate rows\n",
    "\n",
    "- **Description**: In this task,remove any duplicate rows in the data\n",
    "- **Code Instruction**: Complete the function to return a datafarme with de-duplicated dataframe\n",
    "\n",
    "`Ask ChatGPT! : How does duplicate data impact performance of a Logistic Regression model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(df: pd.DataFrame) -> pd.DataFrame :\n",
    "    '''\n",
    "    Complete this function to return a de-duplicated dataframe\n",
    "    '''\n",
    "    \n",
    "    # TODO: remove the rest of code in this function\n",
    "    df = df.drop_duplicates()\n",
    "    return df\n",
    "\n",
    "\n",
    "# Do not change this code\n",
    "row_count = remove_duplicates(train_df).shape[0]\n",
    "print(row_count)\n",
    "remove_duplicates(train_df.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1-4: Fill Missing Values\n",
    "\n",
    "- **Description** : Fill any missing values in the data with column means for numerical cols and column mode for categorical columsn\n",
    "- **Code Instruction**: \n",
    "    1. Use `train_col_miss` to store mean/mode values for each numerical/categorical cols\n",
    "    2. Fill missing values with mean/mode in `train_col_miss`\n",
    "    3. Return a dataframe with missing values filled in\n",
    "\n",
    "`Ask ChatGPT! : How does missing values impact performance of a Logistic Regression model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_col_miss = {}\n",
    "def fill_missing_value(df: pd.DataFrame, train=False) -> pd.DataFrame:\n",
    "    '''\n",
    "    Complete this function to fill missing (if there are)\n",
    "    with the mean value of the column for numerical features, \n",
    "    and model for categorical features\n",
    "\n",
    "    `train_col_mean` is a dictionary where keys are features\n",
    "    and values are mean of field\n",
    "\n",
    "    Hint: Use feats to iterate through columns\n",
    "    '''\n",
    "\n",
    "    # TODO: Solution below, please remove\n",
    "\n",
    "    for col in num_cols:\n",
    "        if train:\n",
    "            train_col_miss[col] = df[col].mean()\n",
    "        \n",
    "        df[col] = df[col].fillna(train_col_miss[col])\n",
    "\n",
    "    for col in cat_cols:\n",
    "        if train:\n",
    "            train_col_miss[col] = df[col].mode()\n",
    "\n",
    "        df[col] = df[col].fillna(train_col_miss[col])\n",
    "\n",
    "    return df\n",
    "\n",
    "# Do not change this line of code\n",
    "fill_missing_value(train_df.copy(), train=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1-5: Identify outliers\n",
    "\n",
    "- **Description**: Compelete the below function to clip outlier using `Tukey Outlier method`. Replace the outlier with mean values calculated before.\n",
    "- **Code Instruction**:\n",
    "    1. Identify the numerical columns\n",
    "    2. Identify upper and lower bound using `Tukey Outlier method`\n",
    "    3. Replace outlier values using mean/model from `train_col_miss` dict defined earlier\n",
    "\n",
    "`Ask ChatGPT! : How does outlier impact performance of a Logistic Regression Model`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_col_bounds = {}\n",
    "def clip_outliers(df: pd.DataFrame, train=False) -> pd.DataFrame:\n",
    "    '''\n",
    "    Complete this function to get lower, upper bounds of each col\n",
    "    Replace low and high with mean values\n",
    "\n",
    "    `train_col_bounds` is a dictionary where key are features\n",
    "    and values are tuple (x,y) x being lower bound and y being higher bound\n",
    "\n",
    "    Hint: Use feats to iterate through columns\n",
    "    '''\n",
    "\n",
    "    # TODO: solution below, please remove\n",
    "    for col in num_cols:\n",
    "        if train:\n",
    "            p25, p75 = df[col].quantile([.25,.75])\n",
    "            iqr = p75 - p25\n",
    "            train_col_bounds[col] = (p25 - 1.5 * iqr, p75 + 1.5 * iqr)\n",
    "\n",
    "        df[col] = df[col].apply(lambda x: train_col_miss[col] if (x < train_col_bounds[col][0] or x >  train_col_bounds[col][0]) else x)\n",
    "    print(train_col_bounds)\n",
    "\n",
    "\n",
    "# Do not change this code\n",
    "clip_outliers(train_df.copy(), train=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1-6: Identify Class Imbalance\n",
    "\n",
    "- **Description**: Complete this function to return the percentage of 0 labels in the data\n",
    "- **Code Instruction**:\n",
    "    1. Copmlete this function to return the percentage of 0-valued labels in the data\n",
    "\n",
    "`Ask ChatGPT! : How does imbalance impact performance of a Logistic Regression Model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_imbalance(df: pd.DataFrame) -> float:\n",
    "    '''\n",
    "    Copmlete this function to return the percentage of 0 labels in the data\n",
    "    '''\n",
    "\n",
    "    # TODO: Solution below, please remove\n",
    "    return df[label].value_counts(normalize=True)[1] * 100\n",
    "\n",
    "test_imbalance(train_df.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering\n",
    "\n",
    "A logistic regression model needs all of its features to be numeric. As such categorical values need to transformed. \n",
    "A common transformation is `One Hot Encoding`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2-1: Encoding categorical variables \n",
    "\n",
    "- **Description**: Use OneHotEncoder to encode categorical values\n",
    "- **Code Instruction**: \n",
    "    1. Create a onehot encoder for each categorical variable, and store the encoder in dict `cat_enc`\n",
    "    2. Transform categorical variables using OneHot Econder\n",
    "    3. For the `label` field, convert 'yes' to 1 and 'no' to 0.\n",
    "    4. Return dataframe with transformed categorical columns alone (dont keep the old ones)\n",
    "\n",
    "`Ask ChatGPT: What are the different types of categorical encoding and advantages of each`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_enc = None\n",
    "def encode_cat(df: pd.DataFrame, train=False):\n",
    "    '''\n",
    "    Use 'OneHotEncoder' to encode categorical values\n",
    "    Remember that the encoder must be saved in `cat_enc` \n",
    "    so that it can used on a test set later\n",
    "\n",
    "    '''\n",
    "\n",
    "    global cat_enc\n",
    "\n",
    "    # TODO: Solution below, remove\n",
    "    if train:\n",
    "        le = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "        le = le.fit(df[cat_cols])\n",
    "        cat_enc = le\n",
    "\n",
    "    le = cat_enc\n",
    "    encoded_df = le.transform(df[cat_cols])\n",
    "\n",
    "    encoded_df = pd.DataFrame(encoded_df, columns=le.get_feature_names_out(cat_cols))\n",
    "\n",
    "    df = pd.concat([df.drop(columns=cat_cols), encoded_df], axis=1)\n",
    "\n",
    "    if train:\n",
    "        df[label] = df[label].apply(lambda x: 0 if x == 'no' else 1)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Do not change this code\n",
    "train_df = pd.read_csv(data_folder + \"train.csv\")\n",
    "train_df = encode_cat(train_df.copy(), train=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2-2: Feature Distribution\n",
    "\n",
    " - **Description**: If values across classes for a feature overlaps it tends to reduce Logistic Regression models predictive power. As such its a good idea to look at the distribution of feature values<br>\n",
    " - **Code Instruction**: \n",
    "    1. Get the column names for numeric fields in the train_df\n",
    "    2. For each field, plot one histogram each for each label value - showing the spread of the values. \n",
    "    3. See if any distribution tend to overlap quite a bit or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Solution below, please remove\n",
    "feats = num_cols\n",
    "print(num_cols)\n",
    "fig, axes = plt.subplots(2, 3, figsize=(12, 6))\n",
    "\n",
    "for i, feat in enumerate(feats):\n",
    "    row = i // 3\n",
    "    col = i % 3\n",
    "    train_df[train_df[label]==0][feat].hist(ax=axes[row, col])\n",
    "    train_df[train_df[label]==1][feat].hist(ax=axes[row+1, col])\n",
    "    axes[row, col].set_title(feat)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2-3 - Feature Correlations for Numerical Features\n",
    "\n",
    "- **Description**: In this task, calculate pairwise correlation between features\n",
    "- **Code Instruction**:\n",
    "    1. Complete the function to calculate all pairwise correlation \n",
    "    (Hint: There is in inubilt-function in `pandas`for this)\n",
    "\n",
    "`Ask ChatGPT! : How do correlated feature impact performance of a Logistic Regression Model.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_corr_num(df: pd.DataFrame) -> None:\n",
    "    '''\n",
    "    Complete the function to calculate all pairwise correlation\n",
    "    '''\n",
    "    \n",
    "    # TODO: Solution below, please remove\n",
    "    return df[num_cols].corr()\n",
    "\n",
    "\n",
    "# Do not change this code\n",
    "calc_corr_num(train_df[feats])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2-4: Feature Correlation for Categorical Features\n",
    "\n",
    "- **Description**: In this task, check if any pair of categorical features are correlated. Since all features are categorical you can use Chi-Square\n",
    "- **Code Instruction**: \n",
    "    1. Complete function to print correlation between all pairs of categorical features\n",
    "\n",
    "\n",
    "`Ask ChatGPT: Is Chi-square reliable when fields have high cardinality`. <br>\n",
    "`Ask ChatGPT: What is the downside of using p-values when doing multiple hypothesis testing?` <br>\n",
    "\n",
    "Based on above answers, and to keep the complexity of the project low - we will skip checking correlation for remaining features pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_corr_cat(df: pd.DataFrame) -> None:\n",
    "    '''\n",
    "    Complete the function to calculate all pairwise correlation\n",
    "    From the output Identify the pair of features that are highly correlated.\n",
    "    '''\n",
    "\n",
    "    # TODO: Solution below, please remove\n",
    "    for feat1 in cat_cols[:-1]:\n",
    "        for feat2 in cat_cols[1:]:\n",
    "\n",
    "            # Create a contingency table\n",
    "            contingency_table = pd.crosstab(df[feat1], df[feat2])\n",
    "\n",
    "            # Perform the Chi-Square test\n",
    "            chi2, p_value, dof, expected = chi2_contingency(contingency_table)\n",
    "            \n",
    "            print(f\"{feat1} {feat2} Chi-Square value: {chi2} P-value: {p_value}\")\n",
    "\n",
    "\n",
    "# Do not change this code\n",
    "temp_df = pd.read_csv(data_folder + 'train.csv')\n",
    "calc_corr_cat(temp_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Understanding Classification Metrics\n",
    "\n",
    "The most common classification metrics are - \n",
    "* Accuraccy\n",
    "* Precision\n",
    "* Recall\n",
    "* F1-Score\n",
    "Let's Ask ChatGPT what there are - <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3-1: Choosing the right metric\n",
    "\n",
    "- **Description**: Now having analyzed the data (from Task 1-6), choose the best metrics for your task. \n",
    "Assume you got the following information from business - \n",
    "* If you prediction someone is going to donate, but they dont - this is huge concern. You want to reduce such `false positives` as much as possible.\n",
    "* If you predict someone is not going to donate, and they do come - it is ok. The blood donation camp can manage.\n",
    "Knowing the above - decide which metric to use. <br>\n",
    "Irrespective of what you use evaluate performance using F1 as well.\n",
    "\n",
    "- **Code Instruction**: \n",
    "    1. Compelete this function to calculate the metric you have chosen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_perf(y_act: list, y_pred: list) -> float:\n",
    "    '''\n",
    "    Compelete this function to calculate the metric you have chosen\n",
    "    '''\n",
    "\n",
    "    # TODO: Solution below, pleas remove\n",
    "    val = precision_score(y_act, y_pred)\n",
    "    return val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build basline model\n",
    "\n",
    "Now, finally we can start training the model. When training an ML model its important to have three datasets\n",
    "* Train dataset - which you use to train the model and learn parameter\n",
    "* Validation dataset - the dataset to use to figure out which parameter are the best\n",
    "* Test dataset - the hidden dataset, that you DO NOT look at. Its only use to estimate the performance in future unseen datasets.\n",
    "\n",
    "### Task 4-1: Preproces the train data to create traininig and validation data\n",
    "\n",
    "- **Description** : In this task, we will create the datasets required for training the model\n",
    "- **Code Instruction**: \n",
    "    1. Load the train dataset \n",
    "    2. ONLY run the de-duplication function on train set\n",
    "    3. Encode categorical values\n",
    "    4. `Stratify` split the train dataset 80:20 to creatin a new train dataset and validation set\n",
    "\n",
    "`Ask ChatGPT: Why is it important to stratify when creating training and validation sets for imbalanced dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(df: pd.DataFrame) -> tuple[np.array, np.array, np.array, np.array, list]:\n",
    "    '''\n",
    "    Remove duplicate data from train file alone \n",
    "    Encode the categorical values\n",
    "    Split train file data into train and valid set (keep in mind what we about imbalance learned in Task 5)\n",
    "        Hint use: train_test_split (set seed to 100), and use the `stratify` field\n",
    "        \n",
    "    Return np.arrays for train features, train labels, valid features, valid labels and list of feature names\n",
    "\n",
    "    '''\n",
    "\n",
    "    # TODO: Solution below, pleas remove\n",
    "    df = remove_duplicates(df)\n",
    "    df = encode_cat(df, train=True)\n",
    "\n",
    "    feats = list(df.columns)\n",
    "    feats.remove(label)\n",
    "    feats.remove('ID')\n",
    "\n",
    "    X = df[feats].values\n",
    "    y = df[label].values\n",
    "\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, \n",
    "                                                          stratify=y, random_state=100)\n",
    "\n",
    "    return X_train, y_train, X_valid, y_valid, feats\n",
    "\n",
    "\n",
    "# Do not change this\n",
    "X_train, y_train, X_valid, y_valid, feats = create_dataset(pd.read_csv('train.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4-2: Build Model\n",
    "\n",
    "- **Description**: Train a basline Logisitc Regression model with default parameters\n",
    "- **Code Instruction**: \n",
    "    1.  Complete this function to train a baseline LogisticRegression Model with default parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_base(X_train: np.array, y_train: np.array) -> LogisticRegression:\n",
    "    '''\n",
    "    Complete this function to\n",
    "    Train a baseline LogisticRegression Model with default parameter\n",
    "    Use random_state = 100 to keep results consistent\n",
    "    '''\n",
    "\n",
    "    # TODO: Solution below, please remove\n",
    "    model = LogisticRegression(random_state=100)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# Do not change this\n",
    "model = train_base(X_train, y_train)\n",
    "\n",
    "pred = model.predict(X_train)\n",
    "print(\"Train Performance\")\n",
    "print(\"Selected Metric: \", calc_perf(y_train, pred), \"F1-Score: \", f1_score(y_train, pred))\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "pred = model.predict(X_valid)\n",
    "print(\"Validation Performance\")\n",
    "print(\"Selected Metric: \", calc_perf(y_valid, pred),\"F1-Score: \", f1_score(y_valid, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.Improving baseline model\n",
    "\n",
    "### Task 5-1: Fix imbalance to improve performance\n",
    "\n",
    "- **Description**: Use sampling to balance the number positive and negative samples in the data\n",
    "- **Code Instruction**: Complete the following function to\n",
    "    1. Remove duplicate rows in data\n",
    "    2. Balance the number of positive and negative samples in train_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rebalance_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    '''\n",
    "    Write a function to \n",
    "    (a) remove duplicate rows in data\n",
    "    (b) balance the number of positive and negative samples in train_data\n",
    "    Hint: Use downsampling, use random_state = 100\n",
    "    Hint: Dont forget to reset index after creating a new dataframe\n",
    "\n",
    "    Return balance dataframe\n",
    "    '''\n",
    "\n",
    "    # TODO: Solution below, please remove\n",
    "    df = remove_duplicates(df)\n",
    "\n",
    "    pos_df = df[df[label] == 'yes']\n",
    "    neg_df = df[df[label] == 'no']\n",
    "\n",
    "    df = pd.concat([neg_df.sample(frac=0.4, random_state=100), pos_df]).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "# Do not change the following code\n",
    "# load the data\n",
    "train_df = pd.read_csv('train.csv')\n",
    "print(train_df.shape, train_df[label].value_counts(normalize=True))\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "# balance the data\n",
    "train_df = rebalance_df(train_df)\n",
    "print(\"Balanced\", train_df.shape, train_df[label].value_counts(normalize=True))\n",
    "\n",
    "# check the performance with rebalance dataset\n",
    "print(\"\\n\\n\")\n",
    "X_train, y_train, X_valid, y_valid, feats = create_dataset(train_df)\n",
    "model = train_base(X_train, y_train)\n",
    "print(X_train.shape)\n",
    "\n",
    "pred = model.predict(X_train)\n",
    "print(\"Train Performance\")\n",
    "print(\"Selected Metric: \", calc_perf(y_train, pred), \"F1-Score: \", f1_score(y_train, pred))\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "pred = model.predict(X_valid)\n",
    "print(\"Validation Performance\")\n",
    "print(\"Selected Metric: \", calc_perf(y_valid, pred),\"F1-Score: \", f1_score(y_valid, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5-2:Improve model using Class Imbalance\n",
    "\n",
    "- **Description**: Another way to combat data imbalance is configuring class weights in the Logistic Regression model <br>\n",
    "- **Code Instruction**: Write the function to train a Logistic Regression model\n",
    "    1. Use in-built parameters of Logistic Regression to weight minority mistake more\n",
    "\n",
    "`Ask ChatGPT: Why does one of the methods perform better than the other.`\n",
    "It could be related to the how the feature distribution for both class overlap as seen in Task 2-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_tune_model(X_train: np.array, y_train: np.array):\n",
    "    '''\n",
    "    Write the function to train a Logistic Regression model\n",
    "    and use `class_weight` parameter\n",
    "    Use random_state = 100 to keep results consistent\n",
    "    '''\n",
    "\n",
    "    # TODO: Solution below, please remove code\n",
    "    model = LogisticRegression(class_weight={0:0.25, 1:0.75 }, random_state=100)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# do no change the following code\n",
    "train_df = pd.read_csv(data_folder + 'train.csv')\n",
    "X_train, y_train, X_valid, y_valid, feats = create_dataset(train_df)\n",
    "\n",
    "model = train_tune_model(X_train, y_train)\n",
    "\n",
    "pred = model.predict(X_train)\n",
    "print(\"Train Performance\")\n",
    "print(\"Selected Metric: \", calc_perf(y_train, pred), \"F1-Score: \", f1_score(y_train, pred))\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "pred = model.predict(X_valid)\n",
    "print(\"Validation Performance\")\n",
    "print(\"Selected Metric: \", calc_perf(y_valid, pred),\"F1-Score: \", f1_score(y_valid, pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5-3: Normalizing Features\n",
    "\n",
    "- **Describe**: Normalize features to see how it impact perforamance for the Log.Reg model from the task\n",
    "- **Code Instruction** Complete the function to\n",
    "    1. Use standard scaling to normalize features \n",
    "\n",
    "`Ask ChatGPT what type of feature scaling is best for Logistic Regression and why`\n",
    "\n",
    "### Bonus [Optional] Task\n",
    "Fine-tune the Logistic Regression Model.\n",
    "Some of the parameters you may want to experiment with are - solver, penatly and C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_tune_model(X_train: np.array, y_train: np.array, scaler):\n",
    "    '''\n",
    "    Complete this function to normalize features \n",
    "    X_train: is the train features values\n",
    "    y_train: is train labels\n",
    "    scaler: The scaler you have chosen\n",
    "    '''\n",
    "\n",
    "    # TODO: Solution below, remove this code\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    model = LogisticRegression(solver = 'liblinear', penalty='l1', C=0.5)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    return (model, scaler)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Do not change this code\n",
    "train_df = pd.read_csv(\"train.csv\")\n",
    "train_df = rebalance_df(train_df)\n",
    "\n",
    "X_train, y_train, X_valid, y_valid, feats = create_dataset(train_df)\n",
    "model, scaler = train_tune_model(X_train, y_train, scaler)\n",
    "\n",
    "X_valid = scaler.transform(X_valid) # we use the same scaler you have used earlier\n",
    "pred = model.predict(X_valid)\n",
    "print(\"Validation Performance\")\n",
    "print(\"Selected Metric: \", calc_perf(y_valid, pred),\"F1-Score: \", f1_score(y_valid, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5-4: Model Interpretability\n",
    "\n",
    "- **Description**: Extract the coefficient from the model by completing this function\n",
    "- **Code Instruction**: Complete the following function to \n",
    "    1. Print thte coefficient for each feature\n",
    "    2. Print the intercept for the Log.Reg model as well\n",
    "\n",
    "`Ask ChatGPT: How do you interpret coeffcient of a Logistic Regression model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_coeff(model : LogisticRegression, feats: list):\n",
    "    '''\n",
    "    Complete this function to print pair of values\n",
    "    (feature name, coeff value)\n",
    "    '''\n",
    "\n",
    "    # TODO: Remove this code\n",
    "    coeffs = list(model.coef_)[0]\n",
    "    print(coeffs)\n",
    "\n",
    "    for i in range(len(feats)):\n",
    "        print(feats[i], coeffs[i])\n",
    "    \n",
    "    print(\"Intercept\", model.intercept_)\n",
    "\n",
    "\n",
    "get_model_coeff(model, feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Error Analysis and Improving Model\n",
    "\n",
    "This is crucial to identify what went wrong and how it could be improved. Use a confusion matrix to figure out if the model is making more false positive or false negative. <br> Identfiy if certain subset of data is more subsceptible to error (like certain jobs, durations etc).Based on the above insights figure out how the model could be improved. Think first - feature engineering, then improve Log.Reg fitting, and then experimenting with other models. \n",
    "\n",
    "### Task 6-1: Understand model error\n",
    "- **Description**: Get the false positive, false negative etc from the model prediction\n",
    "- **Code Instruction**: \n",
    "    1.Calculate true positive, false positive, false negative and true positive and assign to tn, fp, fn, tp respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual = y_valid\n",
    "pred = model.predict(X_valid)\n",
    "\n",
    "# TODO: Remove code below\n",
    "tn, fp, fn, tp = confusion_matrix(actual, pred).ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6-2: Improve model using feature engineering [Optional]\n",
    "- **Description**: See what new feature you can create from existing features to improve model performance\n",
    "- **Code Instruction**: This is an open ended task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update predictions based on insights from error-analysis\n",
    "# TODO: FOR TESTING - SIMPLY CHECK IF PREDICTIONS ARE BETTER THAN THAT FROM TASK.18\n",
    "pred = model.predict(X_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6-3: Model Experimentation\n",
    "\n",
    "- **Description**: Now we can try-out other models. Here we will be experimenting with RFClassifier. Its common in the industry to decide to spend 2-3 days on model experimentation. During this time you can experiment with as many models, fine-tuning techniques as possible\n",
    "- **Code Instruction**:\n",
    "    1. Load the train data\n",
    "    2. Rebalance it using `rebalance_df`\n",
    "    3. Create train, validation data using `create_dataset`\n",
    "    4. Fit a RF Model and experiment with parameters to get best performance on validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Remove code below\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "train_df = pd.read_csv(\"train.csv\")\n",
    "train_df = rebalance_df(train_df)\n",
    "\n",
    "X_train, y_train, X_valid, y_valid, feats = create_dataset(train_df)\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "pred = rf_classifier.predict(X_valid)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Validation Performance\")\n",
    "print(\"Selected Metric: \", calc_perf(y_valid, pred),\"F1-Score: \", f1_score(y_valid, pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6-4 - Final Prediction\n",
    "\n",
    "- **Description**:Finally apply all the transformation you deem best on the data and the best model you found to get prediction on test datasets\n",
    "- **Code Instruction**:\n",
    "    1. Load test data\n",
    "    2. Look at code written till now - missing value, outliers, encoding, scaling - apply all\n",
    "    3. Predict the class using the best model trained from earlier tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_df = pd.read_csv(data_folder + 'test.csv')\n",
    "\n",
    "# TODO: Remove Solution below\n",
    "test_df = encode_cat(test_df, train=False)\n",
    "test_df = test_df.drop(columns=['ID'])\n",
    "test_arr = scaler.transform(test_df.values)\n",
    "pred = model.predict(test_arr)\n",
    "print(pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
